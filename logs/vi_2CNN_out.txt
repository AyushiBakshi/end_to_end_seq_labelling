Found 17493 unique words (203621 in total)
Found 75 unique characters
Found 19 unique named entity tags
14041 / 3250 / 3453 sentences in train / dev / test.
Loaded 400001 pretrained embeddings.
word_to_id:  17493
Initialising model for Char Mode: CNN , Encoder Mode: CNN2, CRF: 1 
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:136: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.
  nn.init.uniform(input_embedding, -bias, bias)
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:594: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(self.conv1.weight)
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:595: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(self.conv2.weight)
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:143: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.
  nn.init.uniform(input_linear.weight, -bias, bias)
EPOCH: 1
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:840: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)
2000 :  0.8801380127145035
4000 :  0.5025063431083036
6000 :  0.38236216258618294
8000 :  0.28010061431200484
10000 :  0.2695249604385081
12000 :  0.2704736308257793
14000 :  0.23203085753586175
EPOCH: 2
16000 :  0.20608502418891908
18000 :  0.19777536747006333
20000 :  0.20678904573712523
22000 :  0.18312005373153223
24000 :  0.1757083271851927
26000 :  0.1704747233937235
28000 :  0.19455468089352035
EPOCH: 3
30000 :  0.1411792857463195
32000 :  0.14357703167350622
34000 :  0.15422256842227183
36000 :  0.14388904767796679
38000 :  0.14941935433750023
40000 :  0.14030142737252246
42000 :  0.12351979068451693
EPOCH: 4
44000 :  0.10995213318505151
46000 :  0.10821968305838929
48000 :  0.12129690950265974
50000 :  0.11765768443537084
52000 :  0.13933323950090495
54000 :  0.1162480791538331
56000 :  0.11882856992482889
Train: new_F: 0.889087276282787 best_F: -1.0 
Accuracy: 0.8863742318287773
Dev: new_F: 0.7802280103187151 best_F: -1.0 
Accuracy: 0.7713063507732807
Saving Model to  ./models/self-trained-model
Test: new_F: 0.706364428945074 best_F: -1.0 
Accuracy: 0.6941398217957505
EPOCH: 5
58000 :  0.08741351598151942
60000 :  0.08793138646207634
62000 :  0.10333234686372053
64000 :  0.10002205600050973
66000 :  0.10553622264316478
68000 :  0.11317764213824515
70000 :  0.10831228517550667
EPOCH: 6
72000 :  0.08947539280439426
74000 :  0.09292533396208973
76000 :  0.09664038723513853
78000 :  0.08210205562975083
80000 :  0.09051965163254672
82000 :  0.08047467077801071
84000 :  0.09311820847010502
EPOCH: 7
86000 :  0.07288351020146777
88000 :  0.0762592392648592
90000 :  0.07791091147108199
92000 :  0.08603864546537836
94000 :  0.07393455094389342
96000 :  0.09375785302577039
98000 :  0.08290929529287919
EPOCH: 8
100000 :  0.07031698234778572
102000 :  0.074587083334581
104000 :  0.07685440489629033
106000 :  0.08479473587662949
108000 :  0.06911373216753437
110000 :  0.0662552795688672
112000 :  0.060556819719729524
Train: new_F: 0.9243772811679579 best_F: 0.889087276282787 
Accuracy: 0.9253846153846154
Dev: new_F: 0.8077595537424028 best_F: 0.7802280103187151 
Accuracy: 0.7989130434782609
Saving Model to  ./models/self-trained-model
Test: new_F: 0.7185586846248032 best_F: 0.706364428945074 
Accuracy: 0.7082758620689655
EPOCH: 9
114000 :  0.0634798892161931
116000 :  0.06837506666765812
118000 :  0.07234994745375575
120000 :  0.07395589020840307
122000 :  0.06984648505980273
124000 :  0.06791991704283477
126000 :  0.06101329227778763
EPOCH: 10
128000 :  0.04968327001431301
130000 :  0.06525336789112084
132000 :  0.059772531655775785
134000 :  0.07613103749688517
136000 :  0.062108717479312736
138000 :  0.06126138149206755
140000 :  0.05909381413301031
EPOCH: 11
142000 :  0.058123220413603495
144000 :  0.05797652250834749
146000 :  0.06161276595436244
148000 :  0.05828680996256874
150000 :  0.04566186107969599
152000 :  0.06176065795159675
154000 :  0.05634891813872709
EPOCH: 12
156000 :  0.04867199128532768
158000 :  0.05375880931198267
160000 :  0.053083314009404435
162000 :  0.05191486033828978
164000 :  0.06251056458824607
166000 :  0.05850185324151267
168000 :  0.06230192228885491
Train: new_F: 0.9447096939210502 best_F: 0.9243772811679579 
Accuracy: 0.9444278712976774
Dev: new_F: 0.8221093815033713 best_F: 0.8077595537424028 
Accuracy: 0.8129733289430359
Saving Model to  ./models/self-trained-model
Test: new_F: 0.730745747928478 best_F: 0.7185586846248032 
Accuracy: 0.7184016463728349
EPOCH: 13
170000 :  0.052630626152833676
172000 :  0.04567276315276638
174000 :  0.05136864993763975
176000 :  0.05622824552789709
178000 :  0.061203630517200555
180000 :  0.0536635699733856
182000 :  0.05475108973889091
EPOCH: 14
184000 :  0.04618533105546614
186000 :  0.04772148784112924
188000 :  0.05588748233967561
190000 :  0.0529199993075898
192000 :  0.04878012091885116
194000 :  0.05672520435477442
196000 :  0.04845596774571308
Time Taken: 49.70769414504369
Losses: [0.8801380127145035, 0.8801380127145035, 0.5025063431083036, 0.38236216258618294, 0.28010061431200484, 0.2695249604385081, 0.2704736308257793, 0.23203085753586175, 0.20608502418891908, 0.19777536747006333, 0.20678904573712523, 0.18312005373153223, 0.1757083271851927, 0.1704747233937235, 0.19455468089352035, 0.1411792857463195, 0.14357703167350622, 0.15422256842227183, 0.14388904767796679, 0.14941935433750023, 0.14030142737252246, 0.12351979068451693, 0.10995213318505151, 0.10821968305838929, 0.12129690950265974, 0.11765768443537084, 0.13933323950090495, 0.1162480791538331, 0.11882856992482889, 0.08741351598151942, 0.08793138646207634, 0.10333234686372053, 0.10002205600050973, 0.10553622264316478, 0.11317764213824515, 0.10831228517550667, 0.08947539280439426, 0.09292533396208973, 0.09664038723513853, 0.08210205562975083, 0.09051965163254672, 0.08047467077801071, 0.09311820847010502, 0.07288351020146777, 0.0762592392648592, 0.07791091147108199, 0.08603864546537836, 0.07393455094389342, 0.09375785302577039, 0.08290929529287919, 0.07031698234778572, 0.074587083334581, 0.07685440489629033, 0.08479473587662949, 0.06911373216753437, 0.0662552795688672, 0.060556819719729524, 0.0634798892161931, 0.06837506666765812, 0.07234994745375575, 0.07395589020840307, 0.06984648505980273, 0.06791991704283477, 0.06101329227778763, 0.04968327001431301, 0.06525336789112084, 0.059772531655775785, 0.07613103749688517, 0.062108717479312736, 0.06126138149206755, 0.05909381413301031, 0.058123220413603495, 0.05797652250834749, 0.06161276595436244, 0.05828680996256874, 0.04566186107969599, 0.06176065795159675, 0.05634891813872709, 0.04867199128532768, 0.05375880931198267, 0.053083314009404435, 0.05191486033828978, 0.06251056458824607, 0.05850185324151267, 0.06230192228885491, 0.052630626152833676, 0.04567276315276638, 0.05136864993763975, 0.05622824552789709, 0.061203630517200555, 0.0536635699733856, 0.05475108973889091, 0.04618533105546614, 0.04772148784112924, 0.05588748233967561, 0.0529199993075898, 0.04878012091885116, 0.05672520435477442, 0.04845596774571308]
Total Params in the model are: 1922255
[[0, 0, 0], [0.889087276282787, 0.7802280103187151, 0.706364428945074], [0.9243772811679579, 0.8077595537424028, 0.7185586846248032], [0.9447096939210502, 0.8221093815033713, 0.730745747928478]]
Best Val Score: 0.8221093815033713
Best Test Score 0.730745747928478
Prediction:
word : tag
Jay : PER
is : NA
from : NA
India : LOC


Donald : PER
is : NA
the : NA
president : NA
of : NA
USA : LOC