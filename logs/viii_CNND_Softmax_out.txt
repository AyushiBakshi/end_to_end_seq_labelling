Found 17493 unique words (203621 in total)
Found 75 unique characters
Found 19 unique named entity tags
14041 / 3250 / 3453 sentences in train / dev / test.
Loaded 400001 pretrained embeddings.
word_to_id:  17493
Initialising model for Char Mode: CNN , Encoder Mode: CNN_DILATED, CRF: 0 
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:136: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.
  nn.init.uniform(input_embedding, -bias, bias)
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:622: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(self.conv1.weight)
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:623: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(self.conv2.weight)
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:624: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(self.conv3.weight)
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:143: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.
  nn.init.uniform(input_linear.weight, -bias, bias)
EPOCH: 1
/home/arpit9295_2/Ayushi_Workspace/end_to_end_seq_labelling/main.py:838: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)
2000 :  0.1637198454411629
4000 :  0.09331949636749788
6000 :  0.07278317840760191
8000 :  0.07046123828848468
10000 :  0.07242765924452031
12000 :  0.06760390953384507
14000 :  0.06351951688043038
EPOCH: 2
16000 :  0.06089387793607889
18000 :  0.05082616523678971
20000 :  0.05162212004145126
22000 :  0.049000026703265316
24000 :  0.04275756610626897
26000 :  0.05669018915417059
28000 :  0.054314537736493626
EPOCH: 3
30000 :  0.04311670652844831
32000 :  0.045143946919009734
34000 :  0.047845528535552184
36000 :  0.04118152043245879
38000 :  0.03724127852668139
40000 :  0.042556202199316634
42000 :  0.0429008513132753
EPOCH: 4
44000 :  0.03276480705140452
46000 :  0.0389034637359343
48000 :  0.033234405780296745
50000 :  0.0321396996915719
52000 :  0.03376997552134427
54000 :  0.039699447748449246
56000 :  0.034743010784040504
Train: new_F: 0.7265212490359325 best_F: -1.0 
Accuracy: 0.6774229237350642
Dev: new_F: 0.6772908366533864 best_F: -1.0 
Accuracy: 0.631740017487613
Saving Model to  ./models/self-trained-model
Test: new_F: 0.6119136503814316 best_F: -1.0 
Accuracy: 0.5636961722488039
EPOCH: 5
58000 :  0.029622967347746245
60000 :  0.026857291972622103
62000 :  0.031252505589260904
64000 :  0.029652976558142887
66000 :  0.03410703027454437
68000 :  0.035171560415748815
70000 :  0.03233873466385597
EPOCH: 6
72000 :  0.025778055566406603
74000 :  0.029886340659296344
76000 :  0.030890152604421645
78000 :  0.025855668927258005
80000 :  0.025549537769021847
82000 :  0.027242977119289583
84000 :  0.02906238030703085
EPOCH: 7
86000 :  0.024745069030709835
88000 :  0.025416464105640312
90000 :  0.024865868338695585
92000 :  0.026498666204847917
94000 :  0.030882558515381804
96000 :  0.02917502521596616
98000 :  0.021138145186163936
EPOCH: 8
100000 :  0.022526516881834085
102000 :  0.022302052633318168
104000 :  0.025491626369218777
106000 :  0.022906876624625823
108000 :  0.0228640654075922
110000 :  0.021294134537325083
112000 :  0.030160795676408295
Train: new_F: 0.7908420799615414 best_F: 0.7265212490359325 
Accuracy: 0.7457031692668001
Dev: new_F: 0.7131358912245057 best_F: 0.6772908366533864 
Accuracy: 0.6653543307086615
Saving Model to  ./models/self-trained-model
Test: new_F: 0.6242839854780154 best_F: 0.6119136503814316 
Accuracy: 0.5722526253512794
EPOCH: 9
114000 :  0.023995004320385253
116000 :  0.018693994999532667
118000 :  0.022043147844145655
120000 :  0.021529657704506284
122000 :  0.026814516520753635
124000 :  0.022041189599566326
126000 :  0.02038820479963528
EPOCH: 10
128000 :  0.01915458098172859
130000 :  0.020005509144973795
132000 :  0.02351773664141797
134000 :  0.018188335220300538
136000 :  0.022063410444648917
138000 :  0.02476020953767985
140000 :  0.023795272213771066
EPOCH: 11
142000 :  0.018354824290703745
144000 :  0.01839417300513467
146000 :  0.026453457563241682
148000 :  0.020581551914786014
150000 :  0.021289095414089004
152000 :  0.020775204541344836
154000 :  0.019431669590304314
EPOCH: 12
156000 :  0.017858227622299792
158000 :  0.019310135566010297
160000 :  0.01704060297922772
162000 :  0.019805897950408255
164000 :  0.023909540951894518
166000 :  0.019047651617929113
168000 :  0.022217623368035486
Train: new_F: 0.7859592495460964 best_F: 0.7908420799615414 
Accuracy: 0.745817221179984
Dev: new_F: 0.7090937946357723 best_F: 0.7131358912245057 
Accuracy: 0.6705688128470659
Test: new_F: 0.6159306644655386 best_F: 0.6242839854780154 
Accuracy: 0.5756827650054004
EPOCH: 13
170000 :  0.017465533434372446
172000 :  0.01841559039086221
174000 :  0.016858172801567428
176000 :  0.019955687956247786
178000 :  0.018894180693306156
180000 :  0.018140885672670075
182000 :  0.02092626083817214
EPOCH: 14
184000 :  0.01569319537929149
186000 :  0.01708103810945789
188000 :  0.022232935730438996
190000 :  0.01873586232112614
192000 :  0.01600982065068476
194000 :  0.019851559924559874
196000 :  0.018128859034252168
Time Taken: 20.889858663082123
Losses: [0.1637198454411629, 0.1637198454411629, 0.09331949636749788, 0.07278317840760191, 0.07046123828848468, 0.07242765924452031, 0.06760390953384507, 0.06351951688043038, 0.06089387793607889, 0.05082616523678971, 0.05162212004145126, 0.049000026703265316, 0.04275756610626897, 0.05669018915417059, 0.054314537736493626, 0.04311670652844831, 0.045143946919009734, 0.047845528535552184, 0.04118152043245879, 0.03724127852668139, 0.042556202199316634, 0.0429008513132753, 0.03276480705140452, 0.0389034637359343, 0.033234405780296745, 0.0321396996915719, 0.03376997552134427, 0.039699447748449246, 0.034743010784040504, 0.029622967347746245, 0.026857291972622103, 0.031252505589260904, 0.029652976558142887, 0.03410703027454437, 0.035171560415748815, 0.03233873466385597, 0.025778055566406603, 0.029886340659296344, 0.030890152604421645, 0.025855668927258005, 0.025549537769021847, 0.027242977119289583, 0.02906238030703085, 0.024745069030709835, 0.025416464105640312, 0.024865868338695585, 0.026498666204847917, 0.030882558515381804, 0.02917502521596616, 0.021138145186163936, 0.022526516881834085, 0.022302052633318168, 0.025491626369218777, 0.022906876624625823, 0.0228640654075922, 0.021294134537325083, 0.030160795676408295, 0.023995004320385253, 0.018693994999532667, 0.022043147844145655, 0.021529657704506284, 0.026814516520753635, 0.022041189599566326, 0.02038820479963528, 0.01915458098172859, 0.020005509144973795, 0.02351773664141797, 0.018188335220300538, 0.022063410444648917, 0.02476020953767985, 0.023795272213771066, 0.018354824290703745, 0.01839417300513467, 0.026453457563241682, 0.020581551914786014, 0.021289095414089004, 0.020775204541344836, 0.019431669590304314, 0.017858227622299792, 0.019310135566010297, 0.01704060297922772, 0.019805897950408255, 0.023909540951894518, 0.019047651617929113, 0.022217623368035486, 0.017465533434372446, 0.01841559039086221, 0.016858172801567428, 0.019955687956247786, 0.018894180693306156, 0.018140885672670075, 0.02092626083817214, 0.01569319537929149, 0.01708103810945789, 0.022232935730438996, 0.01873586232112614, 0.01600982065068476, 0.019851559924559874, 0.018128859034252168]
Total Params in the model are: 2402694
[[0, 0, 0], [0.7265212490359325, 0.6772908366533864, 0.6119136503814316], [0.7908420799615414, 0.7131358912245057, 0.6242839854780154], [0.7859592495460964, 0.7090937946357723, 0.6159306644655386]]
Best Val Score: 0.7131358912245057
Best Test Score 0.6242839854780154
Prediction:
word : tag
Jay : PER
is : NA
from : NA
India : LOC


Donald : PER
is : NA
the : NA
president : NA
of : NA
USA : LOC